{"metadata":{"kernelspec":{"display_name":"Spark 2.0.0 - Scala 2.11","language":"scala","name":"spark2-scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.11.8"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"markdown","metadata":{},"source":"### Note\n\nPlease view the [README](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials/README.md) to learn about installing, setting up dependencies, and importing notebooks in Zeppelin"},{"cell_type":"markdown","metadata":{},"source":"### Background\n\n#### What are hyperparameters?\nAll those paramters which are external to our model and can't be estimated through data. In other words, external tunable parameter on which our network depends. They could be things like batch size, type of optimizer, weights initialization algorithm, regularization factor and learning rate. To get the maximum out of our networks we need to tune the hyperparameters based on the problem at hand. They make a great effect on the network. For example, if you haven't initialized the weights distribution correctly, the network may take forever to train and in some cases it won't even converge to a good solution and fails to give us good predictions.\n\n--- \n\n#### Goals\n- Some good tips for tuning hyperparameters\n- How to tune our network's hyperparameters using DL4J's library"},{"cell_type":"markdown","metadata":{},"source":"## 1. Tips for tuning hyperparameters\n\n### Things to consider\nThere is no rule of thumb for tuning hyperparameters but the following tips can help a lot:\n\n- Make sure the samples are randomized before feeding into the network because stochastic gradient descent (SGD) depends on it\n- \n\n---\n\n### Possible solutions to common issues\n|---|---|\n|Slow training?|Increase the learning rate|\n|Fluctuating loss?|Reduce the learning rate|\n|Network not converging?|See if the weights are initialized correctly|"},{"cell_type":"markdown","metadata":{},"source":"## 2. Tuning hyperparameters in DL4J\nLet's see how all of this works in DL4J\n\nWhen configuring, most of the details before the list() call are our hyperparameters. For example:\n```\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n            .seed(123)\n            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n            .iterations(1)\n            .learningRate(0.006)\n            .updater(Updater.NESTEROVS).momentum(0.9)\n            .regularization(true).l2(1e-4)\n            .list() //All of the configurations before this line are hyperparameters\n```\n\n---\n\n### Imports"},{"cell_type":"code","execution_count":4,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\r\nimport org.deeplearning4j.nn.api.{Model, OptimizationAlgorithm}\r\nimport org.deeplearning4j.nn.conf.layers.{DenseLayer, OutputLayer}\r\nimport org.deeplearning4j.nn.conf.{MultiLayerConfiguration, NeuralNetConfiguration, Updater}\r\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork\r\nimport org.deeplearning4j.nn.weights.WeightInit\r\nimport org.deeplearning4j.optimize.api.IterationListener\r\nimport org.nd4j.linalg.activations.Activation\r\nimport org.nd4j.linalg.learning.config.Nesterovs\r\nimport org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction"},{"cell_type":"markdown","metadata":{},"source":"### Setting up the network inputs\nWe'll use the __MNIST__ dataset for this tutorial."},{"cell_type":"code","execution_count":6,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"val mnistTrain = new MnistDataSetIterator(32, true, 123)\r\nval mnistTest = new MnistDataSetIterator(32, false, 123)"},{"cell_type":"markdown","metadata":{},"source":"### Creating a simple feed-forward network\nWe'll define a function for configuring a simple feed-forward network and see how it performs with different hyperparameters."},{"cell_type":"code","execution_count":8,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"// The function parameters are hyperparameters that we are going to play with.\r\n// We won't deal with batch size much because that mostly depends on how much gpu or cpu memory you have\r\n\r\ndef buildNetwork(\r\n                  learningRate: Double,\r\n                  regularization: Double,\r\n                  hiddenLayerNodes: Integer,\r\n                  weightInit: WeightInit,\r\n                  activation: Activation,\r\n                  updater: Updater,\r\n                  lossFunction: LossFunction,\r\n                  optimizationAlgorithm: OptimizationAlgorithm\r\n                ): MultiLayerConfiguration = {\r\n    new NeuralNetConfiguration.Builder()\r\n      .seed(123)\r\n      .optimizationAlgo(optimizationAlgorithm)\r\n      .iterations(1)\r\n      .learningRate(learningRate)\r\n      .updater(updater)\r\n      .activation(activation)\r\n      .weightInit(weightInit)\r\n      .regularization(true).l2(regularization)\r\n      .list()\r\n      .layer(0, new DenseLayer.Builder()\r\n        .nIn(784)\r\n        .nOut(hiddenLayerNodes)\r\n        .build())\r\n      .layer(1, new OutputLayer.Builder(lossFunction)\r\n        .nIn(hiddenLayerNodes)\r\n        .nOut(10)\r\n        .activation(Activation.SOFTMAX)\r\n        .build())\r\n      .pretrain(false).backprop(true)\r\n      .build()\r\n}\r\n \r\n// This function will take the model configuration and tell us the general idea of the network's performance   \r\ndef trainModelConfiguration(configuration: MultiLayerConfiguration): Unit = {\r\n    println(\"---------------------------------------\")\r\n    val model = new MultiLayerNetwork(configuration)\r\n    model.init()\r\n    //print the score on the notebook every 100 iteration.\r\n    model.setListeners(new IterationListener {\r\n        override def invoke(): Unit = ???\r\n\r\n        override def iterationDone(model: Model, iteration: Int): Unit = {\r\n          if(iteration % 100 == 0) {\r\n            println(\"Score at iteration \" + iteration + \" is \" + model.score())\r\n          }\r\n        }\r\n\r\n        override def invoked(): Nothing = ???\r\n      })\r\n\r\n    model.fit(mnistTrain)\r\n    val evaluation = model.evaluate(mnistTest)\r\n\r\n    // print the basic statistics about the trained classifier\r\n    println(\"Accuracy: \"+evaluation.accuracy())\r\n    println(\"Precision: \"+evaluation.precision())\r\n    println(\"Recall: \"+evaluation.recall())\r\n    println(\"---------------------------------------\")\r\n}"},{"cell_type":"markdown","metadata":{},"source":"### Creating configurations with different hyperparameters\nNow we'll make a bunch of configurations with different hyperparameters to see how they perform"},{"cell_type":"code","execution_count":10,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"val lowLearningRate = buildNetwork(1e-10,  // Learning Rate (Too low here)\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.XAVIER, // Weights initialization type\r\n        Activation.RELU, // Activations\r\n        Updater.ADAGRAD, // Updater\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm\r\n        \r\nval highLearningRate = buildNetwork(1e10,  // Learning Rate (Too High)\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.XAVIER, // Weights initialization type\r\n        Activation.RELU, // Activations\r\n        Updater.ADAGRAD, // Updater\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm\r\n\r\nval zeroWeightsInit = buildNetwork(0.0006,  // Learning Rate\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.ZERO, // Weights initialization type (All weights zero)\r\n        Activation.RELU, // Activations\r\n        Updater.NONE, // Updater (Nesterovs updater)\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm\r\n        \r\nval betterUpdater = buildNetwork(0.0006,  // Learning Rate\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.XAVIER, // Weights initialization type\r\n        Activation.RELU, // Activations\r\n        Updater.ADAM, // Updater (Nesterovs updater)\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm"},{"cell_type":"code","execution_count":11,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"trainModelConfiguration(lowLearningRate)\ntrainModelConfiguration(highLearningRate)\ntrainModelConfiguration(zeroWeightsInit)\ntrainModelConfiguration(betterUpdater)"},{"cell_type":"markdown","metadata":{},"source":"### What happened here?\n\n- When the learning rate was too low, the network couldn't learn fast enough. The score was decreasing but at an extremely low rate.\n- Similarly when the learning rate was too high, the network was able to get itself to a lowered score but couldn't go lower than that. At this stage, it's better to make a checkpoint of the network and train it more with lowered learning rate.\n- When the weights were initialized to zero, the network didn't have a better point to start with the training so it was looking for an optimal point for lowering the score in later iterations.\n- These settings worked really well with out network configuration which gave better accuracy along with faster training.\n\n\n"},{"cell_type":"markdown","metadata":{},"source":"### What's next?\n\n- Check out all of our tutorials available [on Github](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials). Notebooks are numbered for easy following."},{"cell_type":"markdown","metadata":{},"source":""}]}