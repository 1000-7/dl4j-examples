{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Please view the [README](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials/README.md) to learn about installing, setting up dependencies, and importing notebooks in Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "#### Convolutional Neural Networks (CNN)\n",
    "Mainly neural networks extract features from the data they are fed with. Previously, we dealt with fully-connected feedforward networks which multiply inputs with weights, add biases, learns and adjust them, iteratively. In effect, the network parameters are only concerned with individual features and doesn't concern themselves with surrounding features, relative to the particular feature in focus. But considering surrounding features is important for improving network's performance.\n",
    "\n",
    "To accomplish this, we use convolutional neural networks (CNN for short). CNN use a matrix of weights for producing more general features from the input features. Such a set of weights is called a kernel (also known as filters in 2D convolutions). These kernels are best suited for training with images. They slide over the image pixels in 2D and give responses as another feature matrix. \n",
    "\n",
    "At the last few layers of a CNN we have fully-connected layers which then transforms our learned feature matrix into a set of predictions on which we can analyse our network's performance.\n",
    "\n",
    "--- \n",
    "\n",
    "#### Goals\n",
    "- Learn about how CNNs work\n",
    "- Working with CNN in DL4J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How CNNs work\n",
    "\n",
    "### Basic concept\n",
    "\n",
    "At each convolutional layer, our network learns about general features from input. As we go deeper, the CNN layers computes more general features. For example, the first CNN might be computing edges present in the image. The next layer might learn about shapes generated through the edges. The following image shows a visual representation of features learned at each CNN layer.\n",
    "\n",
    "||||\n",
    "|---|---|---|---|\n",
    "|Features learned by CNN Layers|![Features learned by CNN Layers](http://parse.ele.tue.nl/cluster/0/fig1.png)|[Source](http://parse.ele.tue.nl/cluster/0/fig1.png)|[Site](http://derekjanni.github.io/Easy-Neural-Nets/)|\n",
    "\n",
    "___Figure 1:___ The above network shows how more generalized features are learned at each layer of the network.\n",
    "\n",
    "||||\n",
    "|---|---|---|---|\n",
    "|Basic convolution operation|![Basic convolution operation](http://www.songho.ca/dsp/convolution/files/conv2d_matrix.jpg)|[Source](http://www.songho.ca/dsp/convolution/files/conv2d_matrix.jpg)|[Site](http://www.songho.ca/dsp/convolution/convolution.html)|\n",
    "\n",
    "___Figure 2:___ The above figure shows how a basic convolution operation is computed. It's just a weighted sum of corresponding matrix values (between input and kernel)\n",
    "\n",
    "### CNN related terms\n",
    "\n",
    "- ___Stride___\n",
    "    _It tells the convolutional layer how many columns or rows (or both) it should skip while sliding the kernel over the inputs. This results in decreasing the output volume. Which further results in lesser network computations._\n",
    "\n",
    "- ___Padding___\n",
    "    _It tells us how many zeros to append at the matrix's boundary. This also help us in controlling the output volume._\n",
    "\n",
    "- ___Pooling___\n",
    "    _Pooling layer (also subsampling layer) also lets us reduce the output volume by appling different types of filtering or other mathematical operations to the output convolutional responses. Pooling has different types - such as, max pooling, average pooling, L2-norm. Max pooling is the most commonly used pooling type. It gets the maximum of all the values covered by the kernel specified._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN in DL4J\n",
    "Let's see how all of this works in DL4J\n",
    "\n",
    "You can build a convolutional layer in DL4J as:\n",
    "```\n",
    "val convLayer = new ConvolutionLayer.Builder(Array[Int](5, 5), Array[Int](1, 1), Array[Int](0, 0)).name(\"cnn\").nOut(50).biasInit(0).build\n",
    "```\n",
    "Here the convolutional layer is created with a kernel size of _5x5_, _1x1_ stride and a padding of _0x0_ with _50_ kernels and all biases initialized to _0_\n",
    "__Note:__ Kernels also has a depth. The depth is equal to the depth of inputs coming from the previous layer. So, on the first layer, this kernel would have a size of 5x5x3 with 3-channel RGB image being fed as an input.\n",
    "\n",
    "The output for a convolutional layer is of the shape (WoxHoxDo), where:\n",
    "__Wo=(W−Fw+2Ph)/Sh+1__ -> __W__ is the input width, __Fw__ is the kernel width, __Ph__ is the horizontal padding, __Sh__ is the horizontal stride\n",
    "__Ho=(H−Fh+2Pv)/Sv+1__ -> __H__ is the input height, __Fh__ is the kernel height, __Pv__ is the vertical padding, __Sv__ is the vertical stride\n",
    "__Do=K__ -> __K__ is the number of kernels applied\n",
    "\n",
    "For a pooling layer we can do something like this:\n",
    "```\n",
    "val poolLayer = new SubsamplingLayer.Builder(Array[Int](2, 2), Array[Int](2, 2)).name(\"maxpool\").build\n",
    "```\n",
    "The default type of pooling is max pooling. Here we're building a pooling layer with a kernel size of _2x2_ and a stride of _2x2_\n",
    "\n",
    "The output is of the shape (WoxHoxDo), for a pooling layer, where: \n",
    "__Wo = (W−Fw)/Sh+1__ -> __W__ is the input width, __Fw__ is the kernel width, __Sh__ is the horizontal stride\n",
    "__Ho = (H−Fh)/Sv+1__ -> __H__ is the input height, __Fh__ is the kernel height, __Sv__ is the vertical stride\n",
    "__Do = Di__ -> __Di__ is the input depth\n",
    "\n",
    "Also, we have to configure how we pass inputs to it. If it's already an image with multiple channels, we use:\n",
    "```\n",
    "val conf = new NeuralNetConfiguration.Builder()\n",
    "// Hyperparameters code here\n",
    ".list\n",
    "// Layers code here\n",
    ".setInputType(InputType.convolutional(32, 32, 3)) // Setting our input type here (32x32x3 image)\n",
    ".build\n",
    "```\n",
    "\n",
    "Otherwise, if it's a linear array of inputs, we can do something like this:\n",
    "```\n",
    "val conf = new NeuralNetConfiguration.Builder()\n",
    "// Hyperparameters code here\n",
    ".list\n",
    "// Layers code here\n",
    ".setInputType(InputType.convolutionalFlat(28, 28, 1)) // Setting our input type here (784 linear array to 28x28x1 input)\n",
    ".build\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import java.{lang, util}\n",
    "\n",
    "import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\n",
    "import org.deeplearning4j.nn.api.{Model, OptimizationAlgorithm}\n",
    "import org.deeplearning4j.nn.conf.inputs.InputType\n",
    "import org.deeplearning4j.nn.conf.layers.{ConvolutionLayer, DenseLayer, OutputLayer, SubsamplingLayer}\n",
    "import org.deeplearning4j.nn.conf.{LearningRatePolicy, NeuralNetConfiguration, Updater}\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork\n",
    "import org.deeplearning4j.nn.weights.WeightInit\n",
    "import org.deeplearning4j.optimize.api.IterationListener\n",
    "import org.nd4j.linalg.activations.Activation\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the network inputs\n",
    "We'll use the __MNIST__ dataset for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "// Hyperparameters\n",
    "val seed = 123\n",
    "val epochs = 1\n",
    "val batchSize = 64\n",
    "val learningRate = 0.1\n",
    "val learningRateDecay = 0.1\n",
    "\n",
    "// MNIST Dataset\n",
    "val mnistTrain = new MnistDataSetIterator(batchSize, true, seed)\n",
    "val mnistTest = new MnistDataSetIterator(batchSize, false, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "// Learning rate schedule\n",
    "val lrSchedule: util.Map[Integer, lang.Double] = new util.HashMap[Integer, lang.Double]\n",
    "lrSchedule.put(0, learningRate)\n",
    "lrSchedule.put(1000, learningRate * learningRateDecay)\n",
    "lrSchedule.put(4000, learningRate * Math.pow(learningRateDecay, 2))\n",
    "\n",
    "// Network Configuration\n",
    "val conf = new NeuralNetConfiguration.Builder()\n",
    "    .seed(seed)\n",
    "    .iterations(1)\n",
    "    .regularization(true).l2(0.005)\n",
    "    .activation(Activation.RELU)\n",
    "    .learningRateDecayPolicy(LearningRatePolicy.Schedule)\n",
    "    .learningRateSchedule(lrSchedule)\n",
    "    .weightInit(WeightInit.XAVIER)\n",
    "    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "    .updater(Updater.ADAM)\n",
    "    .list\n",
    "    .layer(0, new ConvolutionLayer.Builder(Array[Int](5, 5), Array[Int](1, 1), Array[Int](0, 0))\n",
    "      .name(\"cnn1\").nOut(50).biasInit(0).build)\n",
    "    .layer(1, new SubsamplingLayer.Builder(Array[Int](2, 2), Array[Int](2, 2)).name(\"maxpool1\").build)\n",
    "    .layer(2, new ConvolutionLayer.Builder(Array(5, 5), Array[Int](1, 1), Array[Int](0, 0))\n",
    "      .name(\"cnn2\").nOut(100).biasInit(0).build)\n",
    "    .layer(3, new SubsamplingLayer.Builder(Array[Int](2, 2), Array[Int](2, 2)).name(\"maxpool2\").build)\n",
    "    .layer(4, new DenseLayer.Builder().nOut(500).build)\n",
    "    .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "      .nOut(10).activation(Activation.SOFTMAX).build)\n",
    "    .backprop(true).pretrain(false)\n",
    "    .setInputType(InputType.convolutionalFlat(28, 28, 1))\n",
    "    .build\n",
    "\n",
    "// Initializing model and setting custom listeners\n",
    "val model = new MultiLayerNetwork(conf)\n",
    "model.init()\n",
    "model.setListeners(new IterationListener {\n",
    "  override def invoke(): Unit = ???   \n",
    "  override def iterationDone(model: Model, iteration: Int): Unit = {\n",
    "    if(iteration % 100 == 0) {\n",
    "      println(\"Score at iteration \" + iteration + \" is \" + model.score())\n",
    "    }\n",
    "  }   \n",
    "  override def invoked(): Nothing = ???\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "(1 to epochs).foreach(epochStep => {\n",
    "  println(\"Epoch: \" + epochStep)\n",
    "  model.fit(mnistTrain) // Training\n",
    "  // print the basic statistics about the trained classifier\n",
    "  println(\"Training Stats for epoch: \" + epochStep + \" -> \" + model.evaluate(mnistTest).stats(true)) // Evaluation\n",
    "  mnistTest.reset()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In short, CNNs give better accuracy than simple fully-connected networks. In the example above, we got nearly 99% accuracy with just a simple CNN on the MNIST dataset. \n",
    "Some famous and successful CNNs to study are AlexNet, LeNet, InceptionNet etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "- Check out all of our tutorials available [on Github](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials). Notebooks are numbered for easy following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0 - Scala 2.11",
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
