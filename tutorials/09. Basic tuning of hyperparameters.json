{"paragraphs":[{"text":"%md\n### Note\n\nPlease view the [README](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials/README.md) to learn about installing, setting up dependencies, and importing notebooks in Zeppelin","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Note</h3>\n<p>Please view the <a href=\"https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials/README.md\">README</a> to learn about installing, setting up dependencies, and importing notebooks in Zeppelin</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510760513543_1134408493","id":"20171020-070156_1850232313","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:17:33+0000","dateFinished":"2017-11-17T08:17:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8362"},{"text":"%md\n\n### Background\n\n#### What are hyperparameters?\nAll those paramters which are external to our model and can't be estimated through data. In other words, external tunable parameter on which our network depends. They could be things like batch size, type of optimizer, weights initialization algorithm, regularization factor and learning rate. To get the maximum out of our networks we need to tune the hyperparameters based on the problem at hand. They make a great effect on the network. For example, if you haven't initialized the weights distribution correctly, the network may take forever to train and in some cases it won't even converge to a good solution and fails to give us good predictions.\n\n--- \n\n#### Goals\n- Some good tips for tuning hyperparameters\n- How to tune our network's hyperparameters using DL4J's library","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Background</h3>\n<h4>What are hyperparameters?</h4>\n<p>All those paramters which are external to our model and can&rsquo;t be estimated through data. In other words, external tunable parameter on which our network depends. They could be things like batch size, type of optimizer, weights initialization algorithm, regularization factor and learning rate. To get the maximum out of our networks we need to tune the hyperparameters based on the problem at hand. They make a great effect on the network. For example, if you haven&rsquo;t initialized the weights distribution correctly, the network may take forever to train and in some cases it won&rsquo;t even converge to a good solution and fails to give us good predictions.</p>\n<hr/>\n<h4>Goals</h4>\n<ul>\n  <li>Some good tips for tuning hyperparameters</li>\n  <li>How to tune our network&rsquo;s hyperparameters using DL4J&rsquo;s library</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1510760513549_1631702974","id":"20171020-070208_2069142559","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:17:33+0000","dateFinished":"2017-11-17T08:17:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8363"},{"text":"%md\n## 1. Tips for tuning hyperparameters\n\n### Things to consider\nThere is no rule of thumb for tuning hyperparameters but the following tips can help a lot:\n\n- Make sure the samples are randomized before feeding into the network because stochastic gradient descent (SGD) depends on it\n- \n\n---\n\n### Possible solutions to common issues\n|---|---|\n|Slow training?|Increase the learning rate|\n|Fluctuating loss?|Reduce the learning rate|\n|Network not converging?|See if the weights are initialized correctly|","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>1. Tips for tuning hyperparameters</h2>\n<h3>Things to consider</h3>\n<p>There is no rule of thumb for tuning hyperparameters but the following tips can help a lot:</p>\n<ul>\n  <li>Make sure the samples are randomized before feeding into the network because stochastic gradient descent (SGD) depends on it</li>\n  <li></li>\n</ul>\n<hr/>\n<h3>Possible solutions to common issues</h3>\n<table>\n  <tbody>\n    <tr>\n      <td>Slow training?</td>\n      <td>Increase the learning rate</td>\n    </tr>\n    <tr>\n      <td>Fluctuating loss?</td>\n      <td>Reduce the learning rate</td>\n    </tr>\n    <tr>\n      <td>Network not converging?</td>\n      <td>See if the weights are initialized correctly</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"jobName":"paragraph_1510839909617_1436031779","id":"20171116-134509_791025875","dateCreated":"2017-11-16T13:45:09+0000","dateStarted":"2017-11-17T08:17:34+0000","dateFinished":"2017-11-17T08:17:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8364"},{"text":"%md\n## 2. Tuning hyperparameters in DL4J\nLet's see how all of this works in DL4J\n\nWhen configuring, most of the details before the list() call are our hyperparameters. For example:\n```\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n            .seed(123)\n            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n            .iterations(1)\n            .learningRate(0.006)\n            .updater(Updater.NESTEROVS).momentum(0.9)\n            .regularization(true).l2(1e-4)\n            .list() //All of the configurations before this line are hyperparameters\n```\n\n---\n\n### Imports","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{"0":{"graph":{"mode":"table","height":386.188,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>2. Tuning hyperparameters in DL4J</h2>\n<p>Let&rsquo;s see how all of this works in DL4J</p>\n<p>When configuring, most of the details before the list() call are our hyperparameters. For example:</p>\n<pre><code>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n            .seed(123)\n            .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n            .iterations(1)\n            .learningRate(0.006)\n            .updater(Updater.NESTEROVS).momentum(0.9)\n            .regularization(true).l2(1e-4)\n            .list() //All of the configurations before this line are hyperparameters\n</code></pre>\n<hr/>\n<h3>Imports</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1510760513549_-1361895586","id":"20171020-070710_1843650237","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:17:34+0000","dateFinished":"2017-11-17T08:17:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8365"},{"text":"import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\r\nimport org.deeplearning4j.nn.api.{Model, OptimizationAlgorithm}\r\nimport org.deeplearning4j.nn.conf.layers.{DenseLayer, OutputLayer}\r\nimport org.deeplearning4j.nn.conf.{MultiLayerConfiguration, NeuralNetConfiguration, Updater}\r\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork\r\nimport org.deeplearning4j.nn.weights.WeightInit\r\nimport org.deeplearning4j.optimize.api.IterationListener\r\nimport org.nd4j.linalg.activations.Activation\r\nimport org.nd4j.linalg.learning.config.Nesterovs\r\nimport org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator\nimport org.deeplearning4j.nn.api.{Model, OptimizationAlgorithm}\nimport org.deeplearning4j.nn.conf.layers.{DenseLayer, OutputLayer}\nimport org.deeplearning4j.nn.conf.{MultiLayerConfiguration, NeuralNetConfiguration, Updater}\nimport org.deeplearning4j.nn.multilayer.MultiLayerNetwork\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.optimize.api.IterationListener\nimport org.nd4j.linalg.activations.Activation\nimport org.nd4j.linalg.learning.config.Nesterovs\nimport org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction\n"}]},"apps":[],"jobName":"paragraph_1510760513549_-838440404","id":"20171020-071303_1517144370","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:17:34+0000","dateFinished":"2017-11-17T08:17:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8366"},{"text":"%md\n\n### Setting up the network inputs\nWe'll use the __MNIST__ dataset for this tutorial.","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Setting up the network inputs</h3>\n<p>We&rsquo;ll use the <strong>MNIST</strong> dataset for this tutorial.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510855934054_-1241776130","id":"20171116-181214_1098994224","dateCreated":"2017-11-16T18:12:14+0000","dateStarted":"2017-11-17T08:17:55+0000","dateFinished":"2017-11-17T08:17:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8367"},{"text":"val mnistTrain = new MnistDataSetIterator(32, true, 123)\r\nval mnistTest = new MnistDataSetIterator(32, false, 123)","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"mnistTrain: org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator@55d7d878\nmnistTest: org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator = org.deeplearning4j.datasets.iterator.impl.MnistDataSetIterator@2c4af0fc\n"}]},"apps":[],"jobName":"paragraph_1510856082240_-1185835710","id":"20171116-181442_125385967","dateCreated":"2017-11-16T18:14:42+0000","dateStarted":"2017-11-17T08:17:55+0000","dateFinished":"2017-11-17T08:17:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8368"},{"text":"%md\n\n### Creating a simple feed-forward network\nWe'll define a function for configuring a simple feed-forward network and see how it performs with different hyperparameters.","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating a simple feed-forward network</h3>\n<p>We&rsquo;ll define a function for configuring a simple feed-forward network and see how it performs with different hyperparameters.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510760513550_53678775","id":"20171020-072208_966782035","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:17:55+0000","dateFinished":"2017-11-17T08:17:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8369"},{"text":"// The function parameters are hyperparameters that we are going to play with.\r\n// We won't deal with batch size much because that mostly depends on how much gpu or cpu memory you have\r\n\r\ndef buildNetwork(\r\n                  learningRate: Double,\r\n                  regularization: Double,\r\n                  hiddenLayerNodes: Integer,\r\n                  weightInit: WeightInit,\r\n                  activation: Activation,\r\n                  updater: Updater,\r\n                  lossFunction: LossFunction,\r\n                  optimizationAlgorithm: OptimizationAlgorithm\r\n                ): MultiLayerConfiguration = {\r\n    new NeuralNetConfiguration.Builder()\r\n      .seed(123)\r\n      .optimizationAlgo(optimizationAlgorithm)\r\n      .iterations(1)\r\n      .learningRate(learningRate)\r\n      .updater(updater)\r\n      .activation(activation)\r\n      .weightInit(weightInit)\r\n      .regularization(true).l2(regularization)\r\n      .list()\r\n      .layer(0, new DenseLayer.Builder()\r\n        .nIn(784)\r\n        .nOut(hiddenLayerNodes)\r\n        .build())\r\n      .layer(1, new OutputLayer.Builder(lossFunction)\r\n        .nIn(hiddenLayerNodes)\r\n        .nOut(10)\r\n        .activation(Activation.SOFTMAX)\r\n        .build())\r\n      .pretrain(false).backprop(true)\r\n      .build()\r\n}\r\n \r\n// This function will take the model configuration and tell us the general idea of the network's performance   \r\ndef trainModelConfiguration(configuration: MultiLayerConfiguration): Unit = {\r\n    println(\"---------------------------------------\")\r\n    val model = new MultiLayerNetwork(configuration)\r\n    model.init()\r\n    //print the score on the notebook every 100 iteration.\r\n    model.setListeners(new IterationListener {\r\n        override def invoke(): Unit = ???\r\n\r\n        override def iterationDone(model: Model, iteration: Int): Unit = {\r\n          if(iteration % 100 == 0) {\r\n            println(\"Score at iteration \" + iteration + \" is \" + model.score())\r\n          }\r\n        }\r\n\r\n        override def invoked(): Nothing = ???\r\n      })\r\n\r\n    model.fit(mnistTrain)\r\n    val evaluation = model.evaluate(mnistTest)\r\n\r\n    // print the basic statistics about the trained classifier\r\n    println(\"Accuracy: \"+evaluation.accuracy())\r\n    println(\"Precision: \"+evaluation.precision())\r\n    println(\"Recall: \"+evaluation.recall())\r\n    println(\"---------------------------------------\")\r\n}","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"buildNetwork: (learningRate: Double, regularization: Double, hiddenLayerNodes: Integer, weightInit: org.deeplearning4j.nn.weights.WeightInit, activation: org.nd4j.linalg.activations.Activation, updater: org.deeplearning4j.nn.conf.Updater, lossFunction: org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction, optimizationAlgorithm: org.deeplearning4j.nn.api.OptimizationAlgorithm)org.deeplearning4j.nn.conf.MultiLayerConfiguration\ntrainModelConfiguration: (configuration: org.deeplearning4j.nn.conf.MultiLayerConfiguration)Unit\n"}]},"apps":[],"jobName":"paragraph_1510760513550_1140034948","id":"20171020-071349_473511535","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:17:55+0000","dateFinished":"2017-11-17T08:17:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8370"},{"text":"%md\n\n### Creating configurations with different hyperparameters\nNow we'll make a bunch of configurations with different hyperparameters to see how they perform","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating configurations with different hyperparameters</h3>\n<p>Now we&rsquo;ll make a bunch of configurations with different hyperparameters to see how they perform</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1510903885680_-1676494881","id":"20171117-073125_214586916","dateCreated":"2017-11-17T07:31:25+0000","dateStarted":"2017-11-17T08:17:57+0000","dateFinished":"2017-11-17T08:17:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8371"},{"text":"val lowLearningRate = buildNetwork(1e-10,  // Learning Rate (Too low here)\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.XAVIER, // Weights initialization type\r\n        Activation.RELU, // Activations\r\n        Updater.ADAGRAD, // Updater\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm\r\n        \r\nval highLearningRate = buildNetwork(1e10,  // Learning Rate (Too High)\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.XAVIER, // Weights initialization type\r\n        Activation.RELU, // Activations\r\n        Updater.ADAGRAD, // Updater\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm\r\n\r\nval zeroWeightsInit = buildNetwork(0.0006,  // Learning Rate\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.ZERO, // Weights initialization type (All weights zero)\r\n        Activation.RELU, // Activations\r\n        Updater.NONE, // Updater (Nesterovs updater)\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm\r\n        \r\nval betterUpdater = buildNetwork(0.0006,  // Learning Rate\r\n        1e-4, // Regularization\r\n        1000, // Hidden Nodes\r\n        WeightInit.XAVIER, // Weights initialization type\r\n        Activation.RELU, // Activations\r\n        Updater.ADAM, // Updater (Nesterovs updater)\r\n        LossFunction.NEGATIVELOGLIKELIHOOD, // Loss Function\r\n        OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) // Optimization Algorithm","user":"anonymous","dateUpdated":"2017-11-17T08:17:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":true},"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lowLearningRate: org.deeplearning4j.nn.conf.MultiLayerConfiguration =\n{\n  \"backprop\" : true,\n  \"backpropType\" : \"Standard\",\n  \"cacheMode\" : \"NONE\",\n  \"confs\" : [ {\n    \"cacheMode\" : \"NONE\",\n    \"iterationCount\" : 0,\n    \"l1ByParam\" : { },\n    \"l2ByParam\" : { },\n    \"layer\" : {\n      \"dense\" : {\n        \"activationFn\" : {\n          \"ReLU\" : { }\n        },\n        \"adamMeanDecay\" : \"NaN\",\n        \"adamVarDecay\" : \"NaN\",\n        \"biasInit\" : 0.0,\n        \"biasLearningRate\" : 1.0E-10,\n        \"dist\" : null,\n        \"dropOut\" : 0.0,\n        \"epsilon\" : 1.0E-6,\n        \"gradientNormalization\" : \"None\",\n        \"gradientNormalizationThreshold\" : 1.0,\n        \"iupdater\" : {\n          \"@class\" : \"org.nd4j.linalg.learning.config.AdaGrad\",\n          \"epsilon\" : 1.0E-6,\n          \"learningRate\" : 1...highLearningRate: org.deeplearning4j.nn.conf.MultiLayerConfiguration =\n{\n  \"backprop\" : true,\n  \"backpropType\" : \"Standard\",\n  \"cacheMode\" : \"NONE\",\n  \"confs\" : [ {\n    \"cacheMode\" : \"NONE\",\n    \"iterationCount\" : 0,\n    \"l1ByParam\" : { },\n    \"l2ByParam\" : { },\n    \"layer\" : {\n      \"dense\" : {\n        \"activationFn\" : {\n          \"ReLU\" : { }\n        },\n        \"adamMeanDecay\" : \"NaN\",\n        \"adamVarDecay\" : \"NaN\",\n        \"biasInit\" : 0.0,\n        \"biasLearningRate\" : 1.0E10,\n        \"dist\" : null,\n        \"dropOut\" : 0.0,\n        \"epsilon\" : 1.0E-6,\n        \"gradientNormalization\" : \"None\",\n        \"gradientNormalizationThreshold\" : 1.0,\n        \"iupdater\" : {\n          \"@class\" : \"org.nd4j.linalg.learning.config.AdaGrad\",\n          \"epsilon\" : 1.0E-6,\n          \"learningRate\" : 1...zeroWeightsInit: org.deeplearning4j.nn.conf.MultiLayerConfiguration =\n{\n  \"backprop\" : true,\n  \"backpropType\" : \"Standard\",\n  \"cacheMode\" : \"NONE\",\n  \"confs\" : [ {\n    \"cacheMode\" : \"NONE\",\n    \"iterationCount\" : 0,\n    \"l1ByParam\" : { },\n    \"l2ByParam\" : { },\n    \"layer\" : {\n      \"dense\" : {\n        \"activationFn\" : {\n          \"ReLU\" : { }\n        },\n        \"adamMeanDecay\" : \"NaN\",\n        \"adamVarDecay\" : \"NaN\",\n        \"biasInit\" : 0.0,\n        \"biasLearningRate\" : 6.0E-4,\n        \"dist\" : null,\n        \"dropOut\" : 0.0,\n        \"epsilon\" : \"NaN\",\n        \"gradientNormalization\" : \"None\",\n        \"gradientNormalizationThreshold\" : 1.0,\n        \"iupdater\" : {\n          \"@class\" : \"org.nd4j.linalg.learning.config.NoOp\"\n        },\n        \"l1\" : 0.0,\n        \"l1Bias\" : 0.0,\n        \"...betterUpdater: org.deeplearning4j.nn.conf.MultiLayerConfiguration =\n{\n  \"backprop\" : true,\n  \"backpropType\" : \"Standard\",\n  \"cacheMode\" : \"NONE\",\n  \"confs\" : [ {\n    \"cacheMode\" : \"NONE\",\n    \"iterationCount\" : 0,\n    \"l1ByParam\" : { },\n    \"l2ByParam\" : { },\n    \"layer\" : {\n      \"dense\" : {\n        \"activationFn\" : {\n          \"ReLU\" : { }\n        },\n        \"adamMeanDecay\" : 0.9,\n        \"adamVarDecay\" : 0.999,\n        \"biasInit\" : 0.0,\n        \"biasLearningRate\" : 6.0E-4,\n        \"dist\" : null,\n        \"dropOut\" : 0.0,\n        \"epsilon\" : 1.0E-8,\n        \"gradientNormalization\" : \"None\",\n        \"gradientNormalizationThreshold\" : 1.0,\n        \"iupdater\" : {\n          \"@class\" : \"org.nd4j.linalg.learning.config.Adam\",\n          \"beta1\" : 0.9,\n          \"beta2\" : 0.999,\n          \"eps..."}]},"apps":[],"jobName":"paragraph_1510903993312_-1588008505","id":"20171117-073313_285452901","dateCreated":"2017-11-17T07:33:13+0000","dateStarted":"2017-11-17T08:17:57+0000","dateFinished":"2017-11-17T08:18:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8372"},{"text":"trainModelConfiguration(lowLearningRate)\ntrainModelConfiguration(highLearningRate)\ntrainModelConfiguration(zeroWeightsInit)\ntrainModelConfiguration(betterUpdater)","user":"anonymous","dateUpdated":"2017-11-17T08:17:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------------------------------------\nScore at iteration 0 is 2.2960690632309335\nScore at iteration 100 is 2.3140914383423365\nScore at iteration 200 is 2.3384705313354055\nScore at iteration 300 is 2.3439175234756475\nScore at iteration 400 is 2.3733565861793036\nScore at iteration 500 is 2.2926493658940252\nScore at iteration 600 is 2.297340803537808\nScore at iteration 700 is 2.3165002018912424\nScore at iteration 800 is 2.333408687942002\nScore at iteration 900 is 2.3731548158847713\nScore at iteration 1000 is 2.306908620202151\nScore at iteration 1100 is 2.3572910825973503\nScore at iteration 1200 is 2.3360697762225286\nScore at iteration 1300 is 2.381119290829935\nScore at iteration 1400 is 2.379884366211502\nScore at iteration 1500 is 2.2689597258480028\nScore at iteration 1600 is 2.358719157777574\nScore at iteration 1700 is 2.3792180031021397\nScore at iteration 1800 is 2.3605201153487756\nAccuracy: 0.0857\nPrecision: 0.08297228776331081\nRecall: 0.08643398397636236\n---------------------------------------\n---------------------------------------\nScore at iteration 0 is 2.3726843967880624\nScore at iteration 100 is 1.02008085081765424E17\nScore at iteration 200 is 1.02785680415706752E17\nScore at iteration 300 is 1.03024510430486096E17\nScore at iteration 400 is 1.03236944127673872E17\nScore at iteration 500 is 1.03283170192885024E17\nScore at iteration 600 is 1.03322146858514544E17\nScore at iteration 700 is 1.03635153473291424E17\nScore at iteration 800 is 1.03624141571097104E17\nScore at iteration 900 is 1.0355077644836448E17\nScore at iteration 1000 is 1.0363878177011992E17\nScore at iteration 1100 is 1.03596251375324304E17\nScore at iteration 1200 is 1.03577646828911344E17\nScore at iteration 1300 is 1.0354660004481928E17\nScore at iteration 1400 is 1.0354714628173544E17\nScore at iteration 1500 is 1.0351108360608072E17\nScore at iteration 1600 is 1.03511695972784064E17\nScore at iteration 1700 is 1.03566090320132864E17\nScore at iteration 1800 is 1.03529972745819824E17\nAccuracy: 0.5908\nPrecision: 0.8512422815779834\nRecall: 0.5835147194440281\n---------------------------------------\n---------------------------------------\nScore at iteration 0 is 2.3025853633880615\nScore at iteration 100 is 2.293168067932129\nScore at iteration 200 is 2.3320510387420654\nScore at iteration 300 is 2.3227643966674805\nScore at iteration 400 is 2.2815041542053223\nScore at iteration 500 is 2.292762279510498\nScore at iteration 600 is 2.2937326431274414\nScore at iteration 700 is 2.302072525024414\nScore at iteration 800 is 2.319521188735962\nScore at iteration 900 is 2.3138232231140137\nScore at iteration 1000 is 2.3091049194335938\nScore at iteration 1100 is 2.301398992538452\nScore at iteration 1200 is 2.2904961109161377\nScore at iteration 1300 is 2.3025221824645996\nScore at iteration 1400 is 2.3238391876220703\nScore at iteration 1500 is 2.337585210800171\nScore at iteration 1600 is 2.3040332794189453\nScore at iteration 1700 is 2.3087165355682373\nScore at iteration 1800 is 2.314732789993286\nAccuracy: 0.1135\nPrecision: 0.1135\nRecall: 0.1\n---------------------------------------\n---------------------------------------\nScore at iteration 0 is 2.3541576042618173\nScore at iteration 100 is 1.9827957504431186\nScore at iteration 200 is 1.501670769943042\nScore at iteration 300 is 1.3018418206999978\nScore at iteration 400 is 1.1530052198819354\nScore at iteration 500 is 0.9047085350370428\nScore at iteration 600 is 0.9082650444635163\nScore at iteration 700 is 0.7690812983015125\nScore at iteration 800 is 0.7965177021896163\nScore at iteration 900 is 0.6569192763692852\nScore at iteration 1000 is 0.57448151260298\nScore at iteration 1100 is 0.48179637138862375\nScore at iteration 1200 is 0.5336888509029875\nScore at iteration 1300 is 0.6965271808251733\nScore at iteration 1400 is 0.45778525327654723\nScore at iteration 1500 is 0.4247395650260042\nScore at iteration 1600 is 0.44470718743101856\nScore at iteration 1700 is 0.49606317017096224\nScore at iteration 1800 is 0.3474060053154008\nAccuracy: 0.9043\nPrecision: 0.903473400367921\nRecall: 0.9027690398302493\n---------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1510855681344_1388503488","id":"20171116-180801_313228898","dateCreated":"2017-11-16T18:08:01+0000","dateStarted":"2017-11-17T08:18:12+0000","dateFinished":"2017-11-17T08:35:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8373"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510906851626_1708804725","id":"20171117-082051_1088156104","dateCreated":"2017-11-17T08:20:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9616","text":"%md\n\n### What happened here?\n\n- When the learning rate was too low, the network couldn't learn fast enough. The score was decreasing but at an extremely low rate.\n- Similarly when the learning rate was too high, the network was able to get itself to a lowered score but couldn't go lower than that. At this stage, it's better to make a checkpoint of the network and train it more with lowered learning rate.\n- When the weights were initialized to zero, the network didn't have a better point to start with the training so it was looking for an optimal point for lowering the score in later iterations.\n- These settings worked really well with out network configuration which gave better accuracy along with faster training.\n\n\n","dateUpdated":"2017-11-17T09:09:37+0000","dateFinished":"2017-11-17T09:09:37+0000","dateStarted":"2017-11-17T09:09:37+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>What happened here?</h3>\n<ul>\n  <li>When the learning rate was too low, the network couldn&rsquo;t learn fast enough. The score was decreasing but at an extremely low rate.</li>\n  <li>Similarly when the learning rate was too high, the network was able to get itself to a lowered score but couldn&rsquo;t go lower than that. At this stage, it&rsquo;s better to make a checkpoint of the network and train it more with lowered learning rate.</li>\n  <li>When the weights were initialized to zero, the network didn&rsquo;t have a better point to start with the training so it was looking for an optimal point for lowering the score in later iterations.</li>\n  <li>These settings worked really well with out network configuration which gave better accuracy along with faster training.</li>\n</ul>\n</div>"}]}},{"text":"%md\n\n### What's next?\n\n- Check out all of our tutorials available [on Github](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials). Notebooks are numbered for easy following.","user":"anonymous","dateUpdated":"2017-11-17T08:17:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>What&rsquo;s next?</h3>\n<ul>\n  <li>Check out all of our tutorials available <a href=\"https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials\">on Github</a>. Notebooks are numbered for easy following.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1510760513551_-146811393","id":"20171020-072151_195526063","dateCreated":"2017-11-15T15:41:53+0000","dateStarted":"2017-11-17T08:35:56+0000","dateFinished":"2017-11-17T08:35:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:8374"},{"text":"%md\n","dateUpdated":"2017-11-17T08:17:34+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510760513551_-1719768476","id":"20171020-072158_2072802023","dateCreated":"2017-11-15T15:41:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:8375","user":"anonymous"}],"name":"09. Basic tuning of hyperparameters","id":"2D1X2SEWZ","angularObjects":{"2CZQ7ZY6S:shared_process":[],"2D1CXP1PP:shared_process":[],"2CZ2UUD41:shared_process":[],"2D1AKPRZA:shared_process":[],"2CX1MZBK8:shared_process":[],"2CXVEN2YD:shared_process":[],"2CX4DY6NZ:shared_process":[],"2CXVDPAZN:shared_process":[],"2CXK9GBTB:shared_process":[],"2CXAT5DWH:shared_process":[],"2CXRBFGBY:shared_process":[],"2CZDYJPG9:shared_process":[],"2CY9RSS9K:shared_process":[],"2CZSZQC4M:shared_process":[],"2CYBYW659:shared_process":[],"2CZCD55JD:shared_process":[],"2CYAJA9NF:shared_process":[],"2D1FESVJD:shared_process":[],"2CYGUF5SA:shared_process":[],"2CZ4X76GG:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}