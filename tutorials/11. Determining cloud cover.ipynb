{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Please view the [README](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials/README.md) to learn about installing, setting up dependencies, and importing notebooks in Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "#### Cloud Cover\n",
    "By definition, _cloud cover_ is the portion of the _sky_ covered by _clouds_ when viewed from a particular location (such as a weather station). It is usually measured in a unit called [__Okta__](https://en.wikipedia.org/wiki/Okta). Cloud cover is also helpful in determining sunshine duration as it is inversly related to cloud cover.\n",
    "\n",
    "--- \n",
    "\n",
    "#### Goals\n",
    "- Determining cloud cover in DL4J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Determining cloud cover in DL4J\n",
    "\n",
    "We're going to translate [this code](https://github.com/bpark738/Cloud/tree/master/src/main/java/stat215) into scala and visualize it in zeppelin's notebook format. The code models cloud detection in polar regions based on __radiances__ recorded automatically by the ___MISR sensor___ aboard the NASA satellite, ___Terra___.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN in DL4J\n",
    "- #### Data\n",
    "    You can view or download the data from [here](https://github.com/bpark738/Cloud/tree/master/src/main/resources). The dataset has 9 train/test splits for our convenience.\n",
    "\n",
    "- #### Data features\n",
    "    - 3 satellite [images](https://github.com/bpark738/Cloud/tree/master/images).\n",
    "    - __Expert labels__ used for model training for each point in image (see the images in the table below).\n",
    "    - __NDAI, SD, CORR__, based on [subject knowledge](https://github.com/bpark738/Cloud/blob/master/yu2008.pdf).\n",
    "    - __DF, CF, BF, AF, AN__ [(Radiance angles)](http://www-misr.jpl.nasa.gov/).\n",
    "\n",
    "- #### Images\n",
    "\n",
    "|Image 1|Image 2|Image 3|\n",
    "|---|---|---|\n",
    "|![Image 1](https://raw.githubusercontent.com/bpark738/Cloud/master/images/image1.png)|![Image 2](https://raw.githubusercontent.com/bpark738/Cloud/master/images/image2.png)|![Image 3](https://raw.githubusercontent.com/bpark738/Cloud/master/images/image3.png)|\n",
    "\n",
    "|Figures|\n",
    "|---|\n",
    "|In the above table, the figure shows the regions highlighted though colored labels - ___ice___ as __'red'__, ___clouds___ as __'blue'__ and ___unknowns___ as __'green'__|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import java.io.File\r\n",
    "import java.net.URL\r\n",
    "\r\n",
    "import org.apache.commons.io.FileUtils\r\n",
    "import org.datavec.api.records.reader.impl.csv.CSVRecordReader\r\n",
    "import org.datavec.api.split.FileSplit\r\n",
    "import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator\r\n",
    "import org.deeplearning4j.eval.{Evaluation, ROC}\r\n",
    "import org.deeplearning4j.nn.api.{Model, OptimizationAlgorithm}\r\n",
    "import org.deeplearning4j.nn.conf.{NeuralNetConfiguration, Updater}\r\n",
    "import org.deeplearning4j.nn.conf.layers.{DenseLayer, OutputLayer}\r\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork\r\n",
    "import org.deeplearning4j.nn.weights.WeightInit\r\n",
    "import org.deeplearning4j.optimize.api.IterationListener\r\n",
    "import org.nd4j.linalg.activations.Activation\r\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions.LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val crossValSet = \"1\" // You can use this to specify (1-9) which of the train/test split you want to use.\r\n",
    "\r\n",
    "val seed = 123\r\n",
    "val learningRate = 0.008\r\n",
    "val batchSize = 32\r\n",
    "val nEpochs = 10\r\n",
    "val numInputs = 8\r\n",
    "val numOutputs = 2\r\n",
    "val numHiddenNodes = 50\r\n",
    "val baseUrl = \"https://raw.githubusercontent.com/bpark738/Cloud/master/src/main/resources\"\r\n",
    "val trainFileUrl = baseUrl + \"/train/\" + crossValSet + \".csv\"\r\n",
    "val testFileUrl = baseUrl + \"/test/\" + crossValSet + \".csv\"\r\n",
    "\r\n",
    "val trainFile: File = new File(\"train.csv\")\r\n",
    "val testFile: File = new File(\"test.csv\")\r\n",
    "\r\n",
    "FileUtils.copyURLToFile(new URL(trainFileUrl), trainFile)\r\n",
    "FileUtils.copyURLToFile(new URL(testFileUrl), testFile)\r\n",
    "\r\n",
    "val rrTrain = new CSVRecordReader(1)\r\n",
    "rrTrain.initialize(new FileSplit(trainFile))\r\n",
    "val trainIter = new RecordReaderDataSetIterator(rrTrain, batchSize, 0, 2)\r\n",
    "val rrTest = new CSVRecordReader(1)\r\n",
    "rrTest.initialize(new FileSplit(testFile))\r\n",
    "val testIter = new RecordReaderDataSetIterator(rrTest, batchSize, 0, 2)\r\n",
    "val conf = new NeuralNetConfiguration.Builder()\r\n",
    "    .seed(seed)\r\n",
    "    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\r\n",
    "    .learningRate(learningRate)\r\n",
    "    .updater(Updater.ADAM)\r\n",
    "    .list\r\n",
    "    .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(numHiddenNodes).weightInit(WeightInit.XAVIER).activation(Activation.RELU).build)\r\n",
    "    .layer(1, new OutputLayer.Builder(LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX).nIn(numHiddenNodes).nOut(numOutputs).build)\r\n",
    "    .pretrain(true).backprop(true)\r\n",
    "    .build\r\n",
    "    \r\n",
    "val model = new MultiLayerNetwork(conf)\r\n",
    "model.setListeners(new IterationListener {\r\n",
    "  override def invoke(): Unit = ???   \r\n",
    "  override def iterationDone(model: Model, iteration: Int): Unit = {\r\n",
    "    if(iteration % 2500 == 0) {\r\n",
    "      println(\"Score at iteration \" + iteration + \" is \" + model.score())\r\n",
    "    }\r\n",
    "  }   \r\n",
    "  override def invoked(): Nothing = ???\r\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "(1 to nEpochs).foreach((epoch) => {\r\n",
    "  println(\"Epoch number: \" + epoch)\r\n",
    "  model.fit(trainIter)\r\n",
    "})\r\n",
    "\r\n",
    "println(\"\\nEvaluate model....\")\r\n",
    "println(\"Model evaluation stats:\" + model.evaluate(testIter).stats(true))\r\n",
    "testIter.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val roc = new ROC(100)\r\n",
    "while (testIter.hasNext) {\r\n",
    "  val batch = testIter.next\r\n",
    "  val output = model.output(batch.getFeatures)\r\n",
    "  roc.eval(batch.getLabels, output)\r\n",
    "}\r\n",
    "testIter.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINAL TEST Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "println(\"\\nFINAL TEST AUC: \" + roc.calculateAUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "val df = sc.parallelize(roc.getRocCurve.getFpr zip roc.getRocCurve.getTpr).toDF(\"FPR\",\"TPR\")\n",
    "df.registerTempTable(\"roc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select FPR, TPR from roc order by FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this tutorial, we learned about what cloud cover is and we trained a network in DL4J to determine cloud cover from a bunch of specified features. At the end, we did some visualizations from the results we obtained from the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next?\n",
    "\n",
    "- Check out all of our tutorials available [on Github](https://github.com/deeplearning4j/deeplearning4j/tree/master/dl4j-examples/tutorials). Notebooks are numbered for easy following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.0.0 - Scala 2.11",
   "language": "scala",
   "name": "spark2-scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
